---
layout: post
title: '论文笔记｜Deep Residual Learning for Image Recognition'
categories: '论文笔记'
tags:
  - [论文笔记, 残差网络, 计算机视觉]
---

ResNet是微软研究院何凯明团队在2016年CVPR的最佳论文。它介绍了一种残差网络对传统的深度卷积神经网络进行了改进，让深度神经网络在不断加深的同时，准确率还可以不断提高。

## 背景介绍

深度卷积神经网络为图像识别领域带来了重大的突破。除了更高的准确率之外，这类网络的普遍特征大多是网络的层级在越来越深。但神经网络真的是越深越好吗？首先如果神经网络越来越深，最先出现的问题就是梯度消失和梯度下降问题，这会影响反向传播算法的收敛效率。不过这个问题现在已经可以通过batch normalization来解决。

解决了梯度消失问题，神经网络可以正常的收敛。那么从理论上讲，神经网络深度越大，就越有拟合高阶复杂函数的潜力，准确率应该会更加高。但事实并非如此，实验表明神经网络可能随着深度的加深出现退化的现象。如下图：

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv1/image-20210120143307945.png" alt="image-20210120143307945" style="zoom:87%;" />

从理论上分析，假设存在一个浅层网络，我们在他后面再加入一些网络层让它变为一个新的深层网络。将这个新的网络划分为原本的浅层和新加入的层两部分，如果新增加的网络层能被训练成一个恒等映射( identity mapping )，那么新的深层网络效果至少不应该比原始的浅层效果差，极端情况下新加入的网络层是一个恒等函数，它的效果应该和浅层网络相同。退化问题的出现表明了通过多个非线性层来近似恒等映射其实是困难的。

基于这个观点，本文提出了一种新的改造深度神经网络的方法。让网络直接去拟合潜在的映射关系( underlying mapping )，不如直接让它去拟合输出和输入的差的关系( residual mapping )。如下图：

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv1/image-20210120144932454.png" alt="image-20210120144932454" style="zoom: 67%;" />

继续上文提到的浅层网络加深的例子，假如新加入的层我们要拟合的潜在映射为$H(x)$，我们让这些堆叠的非线性神经网络去拟合残差$F(x)=H(x)-x$。这样原始的映射也可以表示为$H(x)=F(x)+x$。极端情况下，如果$H(x)$是一个恒等映射，那么改造后的新加入层就不需要去拟合一个恒等映射，而是去拟合 $0$ ，求解器可以简单的把多个非线性连接的权重推向零来接近。

<br>

## 深度残差网络

恒等映射一般并不是最优解，但最优解很有可能在恒等映射附近波动，因此如果直接去找恒等映射的抖动，而不是把它当作一个新函数的学习，更可能收敛到最优的情况附近。如下图：

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv1/image-20210120152050899.png" alt="image-20210120152050899" style="zoom: 67%;" />

可以看到残差网络的方差更小，可见卷积层的输出波动更小。

<br>

### 快捷恒等映射

我们每隔几个堆叠层采用残差学习。本文中我们为构建块( blocks )进行正式定义：

$$
y=F(x,\{W_i\})+x
$$

$x,y$是层的输入和输出，函数$F(x,\{W_i\})$表示要学习的残差映射，上图2中的例子有两层，$F=W_2\sigma(W_1x)$，其中$\sigma = Relu$。在实现的时候$F+x$通过快捷连接( Shortcuts )来完成。

快捷连接是简单的加法，并不会引入外部的参数也不会增加计算复杂度( 除了不可避免的加法 )。这有一个前提是$x$和$F$的维度必须是相等的，如果不是这种情况，就必须引入向量$W_s$来匹配维度，这会增加一定的参数：

$$
t=F(x,W_i)+W_sx
$$

论文中提出了两种残差块( residual blocks )的设计。如下图所示：

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv1/image-20210120153404175.png" alt="image-20210120153404175" style="zoom:67%;" />

在训练浅层神经网络的时候，我们选用前面这种，如果网络深度较大的时候，会考虑使用后面这种。这两种块具有相同的参数数量，因此也有相似的时间复杂度。后面这种占据了三层的神经网络，在层数相同时可以大量减少神经网络的参数。

<br>

### 网络架构

我们测试了各种简单/残差网络，并观察到了一致的现象。为了提供讨论的实例，我们描述了ImageNet的两个模型如下。

**简单网络**：我们简单网络的基准（图3，中间）主要受到VGG网络的启发。卷积层主要有3×3的滤波器，并遵循两个简单的设计规则：

1. 对于相同的输出特征图尺寸，层具有相同数量的滤波器；
2. 如果特征图尺寸减半，则滤波器数量加倍，以便保持每层的时间复杂度。我们通过步长为2的卷积层直接执行下采样。网络以全局平均池化层和具有softmax的1000维全连接层结束。图3（中间）的加权层总数为34。

**残差网络**。 基于上述的简单网络，我们插入快捷连接（图3，右），将网络转换为其对应的残差版本。当输入和输出具有相同的维度时（图3中的实线快捷连接）时，可以直接使用恒等快捷连接。当维度增加（图3中的虚线快捷连接）时，我们考虑两个选项：（A）快捷连接仍然执行恒等映射，额外填充零输入以增加维度。此选项不会引入额外的参数；（B）方程（2）中的投影快捷连接用于匹配维度（由1×1卷积完成）。对于这两个选项，当快捷连接跨越两种尺寸的特征图时，它们执行时步长为2。

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv1/image-20210120154519065.png" alt="image-20210120154519065" style="zoom:67%;" />

<br>

## 参考文献

- [Deep Residual Learning for Image Recognition](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385.pdf)

- [Identity Mappings in Deep Residual Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.05027v3.pdf)

- [ResNet pytorch实现](https://www.cnblogs.com/wzyuan/p/9880342.html)

