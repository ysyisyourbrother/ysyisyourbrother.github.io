---
layout: post
title: '论文笔记｜Identity Mappings in Deep Residual Networks'
categories: '论文笔记'
tags:
  - [论文笔记, 残差网络, 计算机视觉]
---

本文是何凯明团队在ResNet发表后，又进一步对其中的shortcut公式理解和网络结构进行了改进和实验。本文证明了前向参数和反向梯度如果直接从Residual Block传递到下一个Residual Block而不用经过ReLU等操作，效果会更好。

## 背景介绍

本文对第一版的定义的通用公式进行了一些简单改动。ResNet第一版主要结构为Residual Units( 在第一版中称之为Residual Blocks )，其通用公式为：

$$
y_l=h(x_l)+F(x_l,W_l)\\
x_{l+1}=f(y_l)
$$

其中：

- $x_l$是第$l$个Residual Unit的输入，$x_{l+1}$其对应的输出
- $h(x_l)$是Residual Units中的shortcut，一般是一个恒等映射$h(x_l)=x_l$
- $F(x_l,W_l)$是Residual Units中$F(x)$，也就是卷积层等神经网络层，其中$W_l$为这个Unit内部的相关权重
- $f$函数为RELU层

<br>

## 深度残差网络分析

对于原始的残差单元进行分析:

$$
y_l=h(x_l)+F(x_l,W_l)\\
x_{l+1}=f(y_l)
$$

如果$f$是一个恒等映射：$x_{l+1}=y_l$，迭代可得：

$$
x_{l+1}=x_{l}+F(x_l,W_l)
$$

递归地，我们可以进一步得到：

$$
x_L=x_l+\sum_{i=l}^{L-1}F(x_i,W_i)
\label{4}
$$

这条公式存在一些很好的性质：

1. 深层单元$L$的输入$x_L$可以被表示为浅层单元$l$的输入$x_l$和一系列残差函数的累加$\sum_{i=l}^{L-1}F$ 。因此这个模型也可以看成是在拟合单元$L$和单元$l$之间的残差。
2. $x_L$是一系列累加所得，而传统的网络( plain network )中$x_L$往往是一些列矩阵的乘积，例如:$\prod_{i=0}^{L-1}W_ix_0$

同时公式$\ref{4}$也体现了很好的反向传播性质，求导可得第$L$层和第$l$层之间更新参数之间的关系：

$$
\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{l}}=\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}} \frac{\partial \mathbf{x}_{L}}{\partial \mathbf{x}_{l}}=\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}}\left(1+\frac{\partial}{\partial \mathbf{x}_{l}} \sum_{i=l}^{L-1} \mathcal{F}\left(\mathbf{x}_{i}, \mathcal{W}_{i}\right)\right) =\frac{\partial \varepsilon}{\partial \mathbf{x}_{L}}+\frac{\partial \varepsilon}{\partial \mathbf{x}_{L}}\left(\frac{\partial}{\partial \mathbf{x}_{l}} \sum_{i=l}^{L-1} F\left(\mathbf{x}_{i}, W_{i}\right)\right)
\label{5}
$$

- 可以看到，$\frac{\partial \varepsilon}{\partial \mathbf{x}_{L}}$是深层神经网络的梯度，在反向传播的时候直接传递给浅层，这表明梯度衰减问题得到了很好的控制

- 由于$\frac{\partial}{\partial x_{l}} \sum_{i=l}^{L-1} F\left(x_{l}, W_{l}\right)$不可能永远为-1，而且**第$ L $层和第$ l $层之间**梯度之商不是一个参数的累乘，因此不会出现梯度消失和爆炸的问题，参数会一直保持更新。

  注意这里说的from any unit to another，而不是说指的相邻的两个unit

<br>

## Identity Skip Connections的重要性

为了证明 $h$ 函数为恒等函数的优越性，本文继续做了实验来证明恒等映射的重要性。

首先将恒等函数进行了一点简单的修改$h(x_l)=\lambda_lx_l$

$$
x_{l+1}=\lambda_lx_l+F(x_l,W_l)
$$

同样求导可以得到$x_L$和$x_l$的关系：

$$
\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{l}}=\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}}\left(\left(\prod_{i=l}^{L-1} \lambda_{i}\right)+\frac{\partial}{\partial \mathbf{x}_{l}} \sum_{i=l}^{L-1} \hat{\mathcal{F}}\left(\mathbf{x}_{i}, \mathcal{W}_{i}\right)\right)
\label{8}
$$

不像公式$\ref{5}$，在公式$\ref{8}$中项$\prod_{i=l}^{L-1} \lambda_{i}$考虑极端情况：当对于所有的都有$\lambda_i>1$的时候，会发生梯度爆炸；当对于所有的$i$都有$\lambda_i<1$的时候，会发生梯度消失问题。这会阻碍反向传播信号从shortcut流动，转而去通过网络层的权重来流动，就会出现实验中表明的优化困难问题。

文章还实验了五种变体，并证明了恒等映射是最优的方法：

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv2/image-20210121153610612.png" alt="image-20210121153610612" style="zoom: 50%;" />

<br>

## 关于激活函数的用法

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv2/image-20210121153916889.png" alt="image-20210121153916889" style="zoom: 50%;" />

在上文的所有推理中，都假设了最后的激活函数$ f(y_l) $是一个恒等映射。但在ResNetv1中它被定义为一个ReLU激活函数。在原始的Residual Unit中，BN被用于每个网络层后，ReLU被应用于BN后，除了最后一个ReLU被应用于函数$ f $后。上图展示了其他的几种网络结构的设计方式。

- **BN after addition**：BN层改变了shortcut上的信息传递并阻碍了反向传播。在训练开始阶段就会出现训练误差下降的困难。
- **ReLU before addition**：这会导致残差网络层的输出 $F(x_l,W_l)$ 的输出结果无法为负数，但本质上 residual 函数的取值范围应该是$(-\infty,+\infty)$。
- **Post-activation or pre-activation?**：经过实验表明两者是等价的。对于一般的神经网络( plain network )，先激活和后激活并没有显著的区别。但加入addition层后，激活的先后位置会对优化过程存在影响。因此本文提出了先激活的观点并分析了两种结构：
  - **ReLU-only pre-activation**：在ResNet-110/164上的表现和 original Residual Unit相似，因为ReLU层没用利用到BN层带来的作用。
  - **full pre-activation**：分类准确率有了显著的提高。可见full pre-activation简化了优化过程，并且又有BN起到正则化作用。

<img src="https://ysyisyourbrother.github.io/images/posts_img/ResNetv2/image-20210121161011927.png" alt="image-20210121161011927" style="zoom:50%;" />