---
layout: post
title: '论文笔记｜In-Edge AI - Intelligentizing Mobile Edge Computing Caching and Communication by FL'
categories: '论文笔记'
tags:
  - [论文笔记, 强化学习, 联邦学习, 边缘计算]
---

近年来，随着移动通讯设备的发展，边缘计算也越来越受到关注。 移动边缘计算(MEC)可以在云数据中心和用户设备之间搭建起桥梁，让用户可以提升云服务的响应速度，处理速度等。本文设计并提出了一种**"In-Edge AI"框架**，它结合了**深度强化学习技术**和**移动边缘系统的联邦学习框架**，来优化移动边缘计算、存储和通信。它的核心就是**让移动设备与边缘节点(Edge Node)交换模型参数**，来不断优化改进模型的推断能力。

<img src="https://ysyisyourbrother.github.io/images/posts_img/InEdgeAI/image-20201117170523699.png" alt="image-20201117170523699" style="zoom:40%;" />

## 背景介绍

一些常见的符号如下：

| notations                               |          |
| --------------------------------------- | -------- |
| Mobile Edge Computing                   | MEC      |
| User Equipments                         | UEs      |
| Internet of Things                      | IoT      |
| Quality-of-service                      | QoS      |
| Mobile network operators                | MNOs     |
| Quality-of-experience                   | QoE      |
| Computation offloading                  | 计算卸载 |
| Cognitive Computing                     | 认知运算 |
| Independent and Identically Distributed | IID      |

MEC出现的主要目的是降低主干网络计算和存储资源的负荷，通过**接近UEs**来降低云数据中心传输的延迟。而**具有计算和存储能力的边缘节点**，更是可以解决UEs的一些计算和存储的需求。要满足用户设备的QoE，关键在于用户设备的计算卸载(computation offloading). 计算卸载需要进行传输通信，会占用通信资源。**如何分配通信资源和计算资源是这种通信和计算结合的系统的一个核心问题**。

本文使用深度强化学习来联合解决通信和计算资源，并在两个例子上进行验证，分别是**计算卸载和边缘存储**(edge caching)。而**联邦学习则作为DRL agents的训练框架**。

整个过程主要分为以下三个部分：

<img src="https://ysyisyourbrother.github.io/images/posts_img/InEdgeAI/image-20201117163236999.png" alt="image-20201117163236999" style="zoom: 50%;" />

- 信息收集(information collecting)：为认知学习收集观测数据，如使用的通信和计算资源、无线网络环境和用户设备的请求频率。
- 认知计算(Cognitive Computing)：结合系统的观测数据进行调度，本文中用的是DRL。
- 请求处理(Request Handling)：MEC系统根据认知计算得到的调度策略，对用户请求进行处理。



## 例一：边缘缓存

边缘存储可以在边缘节点中缓存用户热门的数据，这样当用户请求的时候，无需从云中心服务器重复获取相同的数据。

假如有$F$个热门的存储文件，标记为$F=\{1,...,F\}$，每个文件的热门概率(文件被请求的可能性)标记为$(P_f)_{F\times 1}$，这个概率分布通常用**MZipf**分布来描述。文章假设：

1. 文件热门概率分布变化速度很慢
2. 每个文件都有相同的大小

对于每次请求，DRL agents需要**决定是否要对这个请求目标进行缓存**，如果需要缓存，agents还需要**决定替换掉哪个本地文件**。文中将这个决策看作一个马尔可夫决策过程(MarKov Decision Process)，并使用DRL来解决它。



## 例二：计算卸载

### 通信模型

假设N个**用户设备**$N=\{1,...,N\}$，被B个**基站**$B=\{1,..,B\}$覆盖。用户设备可以选择卸载他们的计算任务到一个边缘节点上，通过无线网络传输；或者直接在设备本地运行。假设每一个**基站**都有有M个**频道**$M = \{1, ... ,M\}$。用户可以选择$a_n \in \{0\} \cup M$来传输数据，当$a_n=0$的时候，就在本地运行。频道状态的变化可以建模为一个有限状态离散时间的马尔可夫链，通过Shannon-Hartley定理评估可达到的数据速率。



### 计算模型

计算任务根据伯努利分布来生成。一个计算任务被表示为$(\mu, v)$，$\mu$表示要输入的计算数据，$v$表示计算所需的CPU周期数。当任务被本地执行，计算运行时间被表示为$d_L=v/f_L$，其中$f_L$表示计算机的计算能力，这由用户设备分配的电量决定。当任务调度去边缘节点执行的时候，运行时间计算为$d_E=v/f_E$，其中$f_E$是边缘节点的计算能力。通常来说$f_E \gg f_L$ 。



### 计算卸载存在的问题

用户设备会根据通信和计算资源决策的**控制状态**$\{c, e\}$(其中$c\in\{0\}\cup\{M\}$，$e$表示用户设备分配给无线通信或本地计算的能源消耗)，来决定要将计算卸载$c>0$还是在本地运行($c=0$)，以此来提高性能表现。

问题可以描述为：用户设备决定无线网络和电量分配决策，根据一个固定的**控制策略**$\Phi=\left(\Phi_{C}(\mathbf{Y}), \Phi_{\mathrm{e}}(\mathbf{Y})\right)$，其中$\mathbf{Y}$是观测的网络状态，包含任务队列，累计电量消耗，无线网络频道占据情况。同时还定义一个函数$u(\mathbf{Y},(c, \mathrm{e}))$来评估QoS。通过DQN训练这个控制策略，我们可以不断优化用户设备的性能。



## In-Edge AI  结合联邦学习

使用**DRL**还存在的一个问题是，如何部署DRL agent。就拿计算卸载这个例子来说：如果**DRL agent在边缘结点上或者在远端的云服务器上训练**，由于MEC系统基于网络通信的特征，存在以下几个问题：

1. 训练数据如果很大，会增加无线网络传输负担。
2. 要上传到云端进行训练的**用户设备直接观测数据存在安全隐私的隐患**
3. 如果用服务端的观测数据来代替在用户设备上直接的观测数据，训练数据和目标的相关性相对较弱。

如果**DRL agents在用户设备本地进行训练**，也存在以下几个缺陷：

1. 用户设备计算能力差，无法进行大规模计算。
2. 在用户设备上训练会造成额外的电量花费。

如果DRL模型庞大，有很多的参数，需要很大的算力，此时**分布式的DRL**就需要被使用上。下图a是集中式的DRL，b是传统分布式的DRL，c是结合了联邦学习的DRL。

![image-20201117195042861](https://ysyisyourbrother.github.io/images/posts_img/InEdgeAI/image-20201117195042861.png)

联邦学习在训练DRLagents时被使用，因为它可以解决以下多个关键的问题：

- **非独立同分布数据：**如Fig3b所示，大多数传统的分布式DRL架构不能解决不平衡的和非IID的数据，也无法解决隐私问题。这是因为用户设备是通过自身的网络环境、计算能力和电量情况来进行决策的。因此**任何个体的观测数据都不能代表整体的数据进行模型训练**。而在**联邦学习**中，可以通过`FedAvg`合并多个个体更新的模型来实现。

- **通信能力限制：**用户设备通常可能突然间掉线，或者处于一个很差的网络通信环境中。不过，使用额外的计算可以减少训练模型所需的通信次数。而且，联邦学习在每次通信只会要求部分用户设备上传他们更新的模型参数，这就可以解决突然掉线的问题。

- **不平衡的数据：**某些用户设备会有更多的计算任务要处理，因此他们会观测到更多的网络状态等训练数据。因此不同用户设备间产生的训练数据也是不平衡，这可以通过`FedAvg algorithm`来解决
- **隐私和安全：**用户设备需要上传到联邦学习系统的信息是改进DRL模型所需的最少量。



## 联邦学习与边缘节点结合

在云端AI和用户设备AI中间，存在很多的边缘结点。在联邦学习的帮助下，边缘结点可以结合云和大量的用户设备，来实现一个强大的AI实体。每个边缘结点都可以系统级的运行模型训练任务，也就是它**优化的模型是全局的**，是整个MEC系统的，而不只是它自身的。

- **从边缘结点到云中心：**

  考虑边缘缓存的场景：每个边缘结点都有一个DRL agents，他们使用自己的本地数据来更新模型，来决策是否要缓存文件。

- **从用户设备到边缘结点：**

  在计算卸载场景中，每个用户设备都需要通过DRL agents决策一个计算任务是否需要卸载到边缘结点，以及通过哪个无线网络频道，以及通信或本地计算需要消耗的电量。因为计算能力的电量的限制，用户设备的计算能力有限，因此我们可以**把所有边缘结点看作是一个大的整体的服务器**，来**为用户设备提供计算和训练服务**。

上面提到的这两种场景的方法其实是类似的。联邦学习系统先实例化一组随机的用户设备集合，然后从一个特定服务器中去下载模型的参数，并使用他们自己的数据对下载的模型进行训练，并**仅仅将新的模型参数上传到特定服务器中**，该特定服务器会收集所有上传的更新模型参数并改进模型。

总的来说，联邦学习让资源受限的边缘结点和用户设备去学习一个共享的模型，并允许他们在本地去训练模型。



## 实用性分析

- **冷启动问题：**DRL如何以随机的模型参数被部署，在一开始的探索模型将进行随机的决策。解决办法是模型部署时不能从最开始训练，我们可以用迁移学习的方法来加强模型训练，基本思想就是可以模拟无线通信环境和用户设备请求，预训练一个有效的离线模型。
- **延迟需求：**如果要达到需求的准确度，模型有可能会比较复杂，需要较长的时间进行训练和推断。如果使用迁移学习，在训练的时候就会少一些训练时间和计算开销。因为**预训练模型的存在**，DRL agents在边缘结点或用户设备上可以**经过比较短时间的训练就达到一个比较满意的准确度**。

如Fig3c所示，文中提出的DRL的神经网络并不复杂，它仅仅是一个**MLP**，在一个隐藏层也只有200个神经元。因此比起CNN、RNN等深度神经网络，这个DRL可以做**很快的推断**，**也不需要很大的算力**，适合一般的用户设备场景。



## In-Edge AI性能分析

<img src="https://ysyisyourbrother.github.io/images/posts_img/InEdgeAI/image-20201118150145450.png" alt="image-20201118150145450" style="zoom:50%;" />

为了研究In-Edge AI结合联邦学习系统的性能表现，文中对计算卸载和边缘缓存两个应用场景分别进行了实验。选取其中三个边缘结点$(E_1,E_2,E_3)$和三个用户设备$(m_1,m_2,m_3)$来研究他们的性能表现。

对于**边缘缓存场景：**随着训练过程，三个边缘的**缓存命中率(hit rates)**逐步提高，并最终维持在一定的区间范围内。

对于**计算卸载场景：**三个用户设备的**平均使用率(average utilities)**随着训练的loss降低而增加，并最终维持在一定的区间范围内。

<img src="https://ysyisyourbrother.github.io/images/posts_img/InEdgeAI/image-20201118151252735.png" alt="image-20201118151252735" style="zoom: 50%;" />

- 在Fig6a中描述了边缘缓存场景下，基于联邦学习的DDQN和中心化的DDQN对比，以及和一些其他缓存策略的对比，比如LRU，FIFO等。可以看到基于联邦学习的DDQN和中心化的DDQN的缓存命中率接近，高于一般的缓存策略。

- 在Fig6b中描述了计算卸载场景，基于联邦学习的DDQN和中心化DDQN对比，还有一些基准的计算卸载策略，比如`mobile execution`，`edge node execution`，和`greedy execution`(用户设备根据最大化`immediate utility`决定是卸载计算还是直接本地运行)。
- 如Fig6c，假如在理想化的网络环境中，所有训练数据都可以在一个时间片内传送到目标，可以看到基于联邦学习的DDQN在一段时间后就和中心化的DDQN效果相仿了。不过在实际情况中，大量的训练数据传送不可避免的会存在传输延迟。
- 如Fig6d，统计了在DRL agent得到充分训练所需要的网络传输量，在联邦学习框架中，每个结点仅需要上传他们的自己训练的模型。而无联邦学习的框架，比如中心化的DRL，用户设备需要上传所有的训练数据集中训练，因此需要大量的通信资源。

不过联邦学习也存在一些问题，因为联邦学习框架中的调度服务器仅仅进行模型的合并，并不进行完整的训练，因此用户设备和边缘结点需要承受比较大的本地训练压力，这一定会造成更多的电量开销和计算负担，这些问题都还有待解决。
