---
layout: post
title: '论文笔记｜Ray: A Distributed Framework for Emerging AI Applications'
categories: '论文笔记'
tags:
  - [论文笔记, ray, 分布式系统]
---

下一代的AI将会是`agent`持续与环境进行交互，并使用交互数据进行模型的训练和优化。和传统的单机迭代训练不同，这类应用程序急需一种新的分布式的框架，并具备可扩展性和毫秒级的响应速度，同时必须还要支持动态执行，因为与环境的交互情况是在实时变化的。为了满足如上种种需求，`Ray`分布式框架由此诞生了。

## 背景介绍

为强化学习模型服务的分布式系统需要满足以下的几个要求：

1. **细粒度(fine-grained)：**和真实环境的交互时，能在毫秒时间内**执行交互动作(render actions)**，并能产生大量的模拟交互数据
2. **异构性(heterogeneity)：**
   - 时间粒度：一次与环境交互的模拟可能是毫秒级也可能需要几小时。
   - 并发执行：比如GPU用来训练，CPU用来模拟。
   - 动态执行：在线与环境交互及训练。

现存的很多为大数据处理设计的分布式框架并不能很好的满足上诉的需求。比如`Map-Reduce`和`Spark`不能支持细粒度的环境交互，一些可以多任务并行的系统比如`CIEL`和`Dask`无法支持分布式的训练及提供模型服务。深度学习训练框架如`Tensorflow`则无法提供环境交互和模型服务功能，而模型服务系统`(model-serving systems)`如`Tensorflow-Serving`则既无法进行训练和交互。

一个很容易想到的方法是将多个系统组合起来使用，比如：使用`Horovod`进行分布式的训练，`Clipper`提供模型服务，`CIEL`进行环境交互。然而由于各个系统间**严重的紧耦合特征**，这个方法没有办法站住脚，因为系统间消息传输的延迟。研究者还提出了一些`one-off systems`，来专门为强化学习提供服务，然而这种方法将大量的工程负担，比如调度，容错等给到了系统使用者身上，**极大的增加了开发工作量**。本文提出了一种通用的集群计算的框架，可以满足强化学习模型模拟，训练和服务的需求。它既可以提供**短时间的、无状态的计算服务(Tasks)，如和环境交互**；也**提供长时间的，有状态的计算(Actors)，如模型训练**。

为了满足性能需求，**Ray将两个在其他框架中一般是中心化的部件进行了分布式处理**：

1. **任务调度器(scheduler)**
2. 维护**计算途径(computation lineage)**的元数据存储区和**数据对象(data object)**的目录。

这允许Ray在短时间内调度百万级的任务并只有微秒级的延迟。Ray还提供了**基于计算途径**的`tasks`和`actors`容错机制，以及**基于冗余的(replication)**的数据存储的容错机制。

然而，Ray的设计初衷只是在强化学习的场景下，支持分布式的模拟、训练和服务，并不是要去取代其他的分布式框架。事实上，在**提供服务场景下**，Ray无法取代其他的服务框架如`Clipper`和`Tensorflow-Serving`，因为这些模型可以解决更全面的模型服务，如部署、测试等问题。除了Ray的灵活性，它也**无法代替很多数据并行框架**，如`Spark`等，因为它目前还缺少很多的API。

<br>

## 系统设计动机和需求

一个强化学习应用需要提供训练(training)、服务(serving)和模拟(simulation)的功能：

- **训练：**分布式的模型训练通常使用**随机梯度下降法(SGD)**，**依靠allreduce aggregation和参数服务器可以进行分布式的训练和聚合**。
- **服务：**可以通过训练出来的模型基于当前状态选择一个动作，最小化选择动作的延迟，同时最大化每单位时间决策的数量。因此需要**多个结点**来提供模型决策服务，实现**负载均衡**等目的。

- **模拟：**大部分强化学习应用通过模拟来评估模型。现有的强化学习算法还无法有效率的采样环境交互数据，因为交互的复杂情况变化很大，比如下棋只需要几毫秒，而自动驾驶汽车则可能需要几分钟。

和有监督的机器学习模型不同，训练和服务可以可以在不同的系统上进行，强化学习模型的三个部件是紧密耦合的，并有严格的延迟要求。因此这样一个系统需要满足如下的这些需求：

- **细粒度和异构的运算：**运算时间可以从几毫秒到几小时。训练可以在不同的硬件上，包括CPU，GPU等。
- **灵活的计算模型：**强化学习模型既需要无状态的，也需要有状态的运算。无状态的计算可以被运行在系统上的任何节点中，因此很适合进行数据处理和与环境交互。而有状态的计算很适合来实现参数服务器。
- **动态执行：**计算的结果可以用来判断未来的计算，比如是否还需要继续的训练或模拟更多的交互数据等。
- **兼容性：**可以无缝的兼容现存的仿真模拟器以及深度学习的框架。

<br>

## 编程和计算模型

### 编程模型

**下表展示了Ray提供的API：**

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201122125732652.png" alt="image-20201122125732652" style="zoom:60%;" />

**Tasks：**一个任务代表一个远程函数运行在一个无状态的`worker`上。当一个远程函数被调用后，会立刻返回一个`future`，可以用`ray.get()`来等待并获取最终结果，也可以直接将`future`传入别的函数中使用。在远程函数中，输出只和输入有关，满足幂等性，这使得**出现异常时重复执行该远程函数即可**。

**Actors：**一个actor代表一个有状态的计算，每个`actor(remote class)`中包含的**函数可以被远程调用并被顺序执行**。函数被执行后同样会返回一个`future`，但不同的是函数会被运行在一个有状态的`worker`上。通过`Class.remote()`生成的**句柄**可以被传递到其他的`actors`或tasks中去调用它的函数方法，

**下表二中总结了`Tasks`和actors性质的对比：**

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201122133654487.png" alt="image-20201122133654487" style="zoom:65%;" />

- Task支持细粒度的负载均衡，通过负载感知(按tasks粒度)进行均衡的调度；输入数据本地化，也就是将task安排在已经包含输入数据的结点上，避免数据传输带来的延迟；而且因为是无状态的，不需要checkpoint来进行状态恢复，只需要重复调用远程函数即可，因此异常恢复的开销比较低。
- actors则可以**提供更快的更细粒度的状态更新**(比如参数服务器中的参数更新)，因为这些更新往往发生在内部状态而不是外部状态，这些内部状态在Ray中一般都需要进行序列化和反序列化(数据通信等需要)。除了实现参数服务器，actors还可以用来**包装第三方的模拟器**和其他难以序列化的数据。

为了满足异构性和灵活性，Ray还从三种不同的角度设计了API：

1. 为了解决同时发生的任务可能不同的运行时间，我们引入了`ray.wait()`，他可以等到前`k`个可用的结果就返回，而不是像`ray.get()`那样要等待所有的结果。
2. 为了解决**资源异构性问题(resource-heterogeneous tasks)**，Ray允许用户指定要使用的GPU、CPU资源
3. 为了提升灵活性，Ray还允许嵌套的远程函数`(nested remote functions)`，意味着远程调用的函数可以再次调用其他的远程函数。

<br>

### 计算模型

Ray 使用了动态的**任务图计算模型`(task graph computation model)`**。本节展示了如何从一个用户程序中构建一个计算图(如下图)。

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201122153150216.png" alt="image-20201122153150216" style="zoom:60%;" />

先不考虑actors，在上图的计算图中存在两种结点，分别是数据对象和tasks。同时也存在两种边，分别是数据边和控制边。

- **数据边**用来标记数据对象和tasks的依赖关系：如果**数据对象D是task T的输出**，则存在一条**数据边从T到D**；如果D是T的输入数据，则存在一条边从D到T。
- **控制边**用来标记**嵌套远程函数**而产生的计算依赖：如果task T1调用了task T2，那么我们增加一条从T1到T2的控制边。

Actor方法的调用也同样可以被表示为计算图中的结点。它和task的结点特征基本相同，只有一个关键的不同：对同一个对象连续调用方法，必须要按照顺序执行，因为**先执行的方法可能改变actor中的状态值**，所以里面存在一个**隐含的依赖关系**。因此新增了第三种数据边：**状态边**。如果同一个actor的方法$M_j$在方法$M_i$后被调用，那么就存在一条从$M_i$到$M_j$的状态边。因此所有调用同一个对象的方法，形成了一个链，连接了所有有状态的结点。

考虑下面的用户代码，就可以构成上面的计算图。

````python
@ray.remote
def create_policy():
# Initialize the policy randomly. 
	return policy

@ray.remote(num_gpus=1) 
class Simulator(object):
    def __init__(self):
    # Initialize the environment. 
    self.env = Environment()

    def rollout(self, policy, num_steps): 
        observations = []
        observation = self.env.current_state() 
        for _ in range(num_steps):
        	action = policy(observation) 
            observation = self.env.step(action) 
            observations.append(observation)
        return observations
    
@ray.remote(num_gpus=2)
def update_policy(policy, *rollouts):
    # Update the policy.
    return policy
  
@ray.remote
def train_policy():
	# Create a policy.
    policy_id = create_policy.remote()
	# Create 10 actors.
	simulators = [Simulator.remote() for _ in range(10)] 
    # Do 100 steps of training.
	for _ in range(100):
		# Perform one rollout on each actor.
		rollout_ids = [s.rollout.remote(policy_id) for s in simulators]
        # Update the policy with the rollouts.
        policy_id = update_policy.remote(policy_id, *rollout_ids)
	return ray.get(policy_id)
````

 <br>

## 系统架构

Ray的架构由两部分组成，一部分是**应用层**实现API，一部分是**系统层**提供的高可拓展性和容错能力。

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201122162850945.png" alt="image-20201122162850945" style="zoom:65%;" />

### 应用层

应用层包含三类的进程：

-  **`Driver`**：执行用户程序的进程。
- **`Worker`**：一个无状态的进程，用来执行`Driver`或另外一个`worker`调用的远程函数。
- **`Actor`**：一个有状态的进程。`actor`一般由`Driver`或`worker`显式的实例化，只能调用`actor`暴露的方法。

<br>

### 系统层

系统层由三个主要部分组成：一个全局的控制存储(global control store)，一个分布式的调度器(distributed scheduler)，一个分布式的对象存储(distributed object store)。

#### 全局控制存储(GCS)

**GCS包含完整的系统控制状态**，它是一个`key-value`存储，包含消息的**发布订阅机制`(pub-sub)`** 。使用分区来达到可扩展性，使用**分区冗余**来提供容错机制。GCS使用一个`redis`来存储每个分区，GCS表由对象和任务ID进行分片从而可以扩展规模，并且每个分片都进行链复制以提高容错能力。

**容错机制**假设当结点失效时需要一个方法来保持**计算途径**的信息。现有的解决方案专注于粗粒度的并行，因此可以使用**单个结点来存储**计算途径的信息，并且不会影响性能，但这种设计对细粒度和动态的场景，比如环境交互模拟。因此本文将持久化的途径信息的存储和其他系统组件分离，允许每个组件独立拓展。

想要保证系统低延迟必须**最小化任务调度的开销**，这里包含决定任务在哪里执行**(`task scheduling`)**，以及调度任务**(`task dispatch`)**，包括去哪里获取输入数据。很多现存的系统将对象的**位置和大小(元数据 `meta data`)**存储在中心化的调度器中，这样的话调度器可能成为性能瓶颈。而且让调度器参与每次数据传输有可能造成比较大的开销。因此，Ray将对象的元数据存储在GCS中而不是在调度器中

总的来说，GCS简化了Ray的整体设计，因为它**使得系统中的每个组件都是无状态的**。



#### 自底向上的分布式调度器(Bottom-Up Distributed Scheduler)

本文设计了**两级调度器**，包含一个全局的调度器和单节点的本地调度器。为了防止全局调度器负载过大，在一个节点(`Driver`或`Worker`)上创建的任务，首先会提交到本地调度器(`local scheduler`)，本地调度器会在本地调度该任务，除非该结点已经超过负荷(比如本地的任务队列已经超过预定义的阈值，或者无法满足任务的需求，比如没有GPU)。如果本地调度器无法调度该任务，就会将该任务传送给全局的调度器。

因为调度器是先尝试本地调度，再传送给中心调度器，因此，可以认为是一个**自底向上的调度器(`Bottom-Up`)**。

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201123134854070.png" alt="image-20201123134854070" style="zoom:65%;" />

全局调度器考虑每个结点的负载和任务的限制来进行调度决策。首先考虑结点是否有足够的资源提供给该任务计算，然后选择估计最短等待时间**(`lowest estimated watiting time`)**的结点来执行，这个时间通过计算队列中所有任务的估计时间总和得到。全局调度器通过心跳(`heartbeats`)来得到每个结点队列的大小和结点资源的可用情况，从GCS获取任务的输入数据的位置和大小。如果全局调度器成为瓶颈，它会自动实例化多个全局调度器，他们都共享GCS中的数据。



#### 分布式内存对象存储(In-Memory Distributed Object Store)

在每个结点上，对象存储通过共享内存(`shared memory`)来实现。这使得在同一个结点上运行的task可以实现零拷贝(`zero-copy`)。数据格式方面，使用的是Apach Arrow。

如果任务的输入数据不在本地存储，输入数据会被复制到本地结点上。同时任务的数据结果会被写到本地的对象存储上。为了保证低延迟，存储对象基本都保存在内存里，如果因为内存大小限制，会通过LRU机制将数据写入磁盘中。

对象存储被限制为不可改变的数据(`immutable data`)。这避免了复杂的一致性协议，因为数据没办法被更新，同时也简化了容错机制



#### 组装系统

下面用一个例子来介绍Ray是如何调度执行函数的：

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201123155901955.png" alt="image-20201123155901955" style="zoom:65%;" />

- step0:远程函数`add`被自动登记到GCS中，并分布到系统中每一个`worker`中。数据a和b分别被存储在结点N1和N2上。
- step1:`Driver`调用`add.remote(a,b)`。driver提交add(a,b)到本地调度器上。
- step2:进一步将`add(a,b)`提交到全局调度器上。
- step3:全局调度器在GCS中查找参数`a`和`b`的地址。
- step4:全局调度器决定将任务在结点N2上执行。
- step5:结点N2的本地调度器检查本地存储对象是否包含参数`a`和`b`
- step6:因为`a`没有存储在N2的本地存储上，它从GCS上查找a的位置。
- step7:得知`a`存储在N1结点后，N2的存储将`a`的值复制到本地，此时所有的参数都已经在本地存在。
- step8:本地调度器调用`add`函数在本地的`worker`上执行
- step9:add函数通过本地的共享内存获取参数`a`和`b`

下图演示了执行Ray是如何返回远程任务的结果：

<img src="https://ysyisyourbrother.github.io/images/posts_img/Ray/image-20201123161413709.png" alt="image-20201123161413709" style="zoom:67%;" />

- step1:**c = ray.get($id_c$)**在N1结点上被执行，Driver去本地存储中查询变量`c`。
- step2:因为本地结点上没有存储c，它去GCS中查询变量c的地址，但发现此时并没有变量c存在，因为c还没有被创建。因此N1的本地存储登记一个`callback`，当变量`c`在GCS中被创建的时候会被触发。
- step3:在N2结点，`add(a,b)`执行完毕，将变量c存储在N2的本地存储中。
- step4:接着N2将变量`c`也添加到GCS中。
- step5:GCS触发了一个`callback`回到N1结点。
- step6:N1结点的本地存储将变量`c`从N2结点中复制过来。
- step7:并返回变量`c`的值给到`ray.get()`的赋值对象。

<br>


## Further Reading

- [Hello Ray!  **Part1:** Ray Core Walkthrough](https://ysyisyourbrother.github.io/Hello-Ray-Part1/)
- [Hello Ray!  **Part2:** Build A Simple RL Demo](https://ysyisyourbrother.github.io/Hello-Ray-Part2/)        
- [Hello Ray!  **Part3:** Parallelize your RL model with ray](https://ysyisyourbrother.github.io/Hello-Ray-Part3/)        
- [论文学习｜Ray: A Distributed Framework for Emerging AI Applications](https://ysyisyourbrother.github.io/Ray-Paper/)   